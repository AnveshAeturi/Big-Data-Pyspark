# Big-Data-Pyspark
#### Simple ETL solutions using Pyspark and Spark-SQL.
1. Iris-ETL
2. Load Strategy **Overwrite**
3. ETL pipeline flows like:
   - Read the CSV file and check the Data Quality
   - Write it to simple CSV and partioned Parquet/CSV file
  
4.Input Data:

![image](https://user-images.githubusercontent.com/26872900/162257073-99befe11-896d-405e-869e-16d71d6f6162.png)

5. Output partioned Data: (partition based on species)

![image](https://user-images.githubusercontent.com/26872900/162257240-c1ca8524-f5d9-482b-aae0-015004d0b23e.png)

6. Folder PATH listing

![image](https://user-images.githubusercontent.com/26872900/162257822-4b4319cb-f420-41b8-b5e7-8564b46e2042.png)

